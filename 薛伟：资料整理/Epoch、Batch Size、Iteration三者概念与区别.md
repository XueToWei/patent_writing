@[toc]

---
在神经网络训练中，有三个概念：Epoch、Batch Size、Iteration。下面对三者的概念与区别进行介绍。

---
# 1 Epoch
Epoch，一次完整训练，即"一代训练"。当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个Epoch。

需要将完整的数据集在同样的神经网络中传递多次，仅仅更新权重一次或者说使用一个Epoch是不够的，选择合适的Epoch的数量取决于数据的多样性等因素。

在不能将数据一次性通过神经网络的时候，就需要将数据集分成几个Batch，见【2 Batch Size】一节。

---
# 2 Batch Size
Batch Size，批量大小，即**一次训练所选取的样本数**。由于在数据很庞大的时候，一次性将数据输入计算机是不可能的，可以把数据分成小块，一块一块的传递给计算机。在小样本数的数据库中，不使用Batch Size是可行的，而且效果也很好。但是一旦是大型的数据库，一次性把所有数据输进网络，肯定会引起内存的爆炸。所以就提出Batch Size的概念。

---
# 3 Iteration
Iteration，算法是迭代的，意思是需要多次使用算法获取结果，以得到最优化结果。迭代是将数据分块后需要完成一个Epoch的次数，即完整的数据集通过了神经网络一次并且返回了一次所需的次数。

在一个Epoch中，Batch数和迭代数是相等的。**Batch数是将数据被分成批次的数量**，需要与批量大小即Batch Size区分开。

---
# 4 示例
比如对于一个有2000个训练样本的数据集，将2000个样本分成大小为500的Batch。那么：
- 完成一个Epoch需要4个Iteration。
- Batch数也为4。
- Batch Size为500。
---
# 5 参考文献

1、[神经网络训练中，傻傻分不清Epoch、Batch Size和迭代](https://www.jiqizhixin.com/articles/2017-09-25-3)

2、[神经网络中Batch Size的理解](https://blog.csdn.net/qq_34886403/article/details/82558399)

---
END
